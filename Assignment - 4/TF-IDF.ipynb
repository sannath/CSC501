{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import demoji\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('IRAhandle_tweets_1.csv',usecols=['content','language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_rs = data[(data['language']=='English') | (data['language']=='Russian') ].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "emojis_list         = []\n",
    "hashtags_list       = []\n",
    "cleaned_tweets_list = []\n",
    "words_list          = []\n",
    "\n",
    "for tweet in tweets_en_rs:\n",
    "    \n",
    "    # urls\n",
    "    tweet = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', tweet, flags=re.MULTILINE)\n",
    " \n",
    "# uncomment\n",
    "#     # emojis\n",
    "#     emojis_in_tweet =  demoji.findall(tweet)\n",
    "#     emojis_list.append(emojis_in_tweet)\n",
    "    \n",
    "    # hashtags\n",
    "    hashtags_in_tweet = re.findall('\\#(\\w*)', tweet)\n",
    "    hashtags_list.append(hashtags_in_tweet)\n",
    "    \n",
    "    # punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    # to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #check for tweet == null before appending \n",
    "    if tweet:\n",
    "        cleaned_tweets_list.append(tweet)\n",
    "    \n",
    "    # stop-words\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tweet_to_words = TreebankWordTokenizer().tokenize(tweet)\n",
    "    words__in_tweet = [words for words in tweet_to_words if words not in stop_words]    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtags_list\n",
    "# len(cleaned_tweets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(cleaned_tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(cleaned_tweets_list)\n",
    " \n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    " \n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# count matrix\n",
    "count_vector=cv.transform(cleaned_tweets_list)\n",
    " \n",
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00529199]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            #### TF-IDF    \n",
    "    \n",
    "\n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    "second_document_vector=tf_idf_vector[1]\n",
    "\n",
    "#print the scores\n",
    "# df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"TF-IDF\"])\n",
    "# df.sort_values(by=[\"TF-IDF\"],ascending=False)\n",
    "\n",
    "# similarity(first_document_vector,second_document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between 2 docs\n",
    "\n",
    "def similarity(doc1,doc2):\n",
    "    doc1 = first_document_vector.toarray()\n",
    "    doc2 = second_document_vector.toarray()\n",
    "    return np.dot(doc1,doc2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #### word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = Word2Vec(words_list,min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pence', 0.908073365688324),\n",
       " ('ferociously', 0.8967700004577637),\n",
       " ('j', 0.89599609375),\n",
       " ('president', 0.8923373222351074),\n",
       " ('rick', 0.8906669616699219),\n",
       " ('message', 0.8905045986175537),\n",
       " ('presidentelect', 0.8782235383987427),\n",
       " ('trumpâ€¦', 0.8765853643417358),\n",
       " ('iowa', 0.8765603303909302),\n",
       " ('trumpsfavoriteheadline', 0.8760970830917358)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(['donald','trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
